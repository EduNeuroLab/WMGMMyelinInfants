{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b95b04-2380-481e-af5f-8e1c215eb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhcp_endpointstocortex_pipeline(subject, session, local_env=False):\n",
    "    \"\"\"\n",
    "    For each `subject`, `session` pair run tractography pipeline.\n",
    "    systemctl --user start docker-desktop \n",
    "    0. Perpare local environment\n",
    "    1. Generate isotropic DWI files\n",
    "    2. Realign DWI to anatomy using rigid transformation\n",
    "    3. Estimate CSD response function\n",
    "    4. Reassign ribbon values\n",
    "    5. Generate five-tissue-type (5TT) segmented tissue image\n",
    "    6. Tractography\n",
    "    7. Create BIDS derivatives\n",
    "    8. Upload to s3\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject : string\n",
    "    session : string\n",
    "    aws_access_key : string\n",
    "    aws_secret_key : string\n",
    "    streamline_count : int\n",
    "    local_env : boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    import subprocess\n",
    "    import os\n",
    "    from os.path import exists, join\n",
    "    import s3fs\n",
    "    import json\n",
    "    from AFQ.definitions.mapping import AffMap\n",
    "    import nibabel as nib\n",
    "    import numpy as np\n",
    "    import os.path as op\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import nipype.interfaces.freesurfer\n",
    "      \n",
    "    fs = s3fs.S3FileSystem()\n",
    "\n",
    "    def print_quiet(string):\n",
    "        \"\"\"\n",
    "        mrtrix doesn't provide a way to conviently supress progress messages without\n",
    "        also supressing informational messages. \n",
    "        \n",
    "        filter stderr/stdout before printing\n",
    "\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # remove any line that looks like a progress message\n",
    "        string = re.sub(r'.*\\[[0-9 ]{3}\\%\\].*\\n?', '', string)\n",
    "\n",
    "        # remove empty lines\n",
    "        string = re.sub(r'^\\n$', '', string)\n",
    "        \n",
    "        # remove any leading or trailing whitespace\n",
    "        string = string.strip()\n",
    "        \n",
    "        if string != \"\":\n",
    "            print(string, flush=True)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 0: Prepare local environment and downlaod data\n",
    "    ###############################################################################\n",
    "    cmd='export SUBJECTS_DIR=/output'\n",
    "    print(cmd)\n",
    "    out = os.system(cmd)\n",
    "    \n",
    "    print('Step 0: Prepare local environment', flush=True)\n",
    "    \n",
    "    os.makedirs(join('input', f'sub-{subject}', f'ses-{session}'), exist_ok=True)\n",
    "    os.makedirs(join('output', f'sub-{subject}', f'ses-{session}'), exist_ok=True)\n",
    "    os.makedirs(join('output', f'sub-{subject}', f'ses-{session}','tracts'), exist_ok=True)   \n",
    "    os.makedirs(join('output', f'sub-{subject}', f'ses-{session}','volume'), exist_ok=True)\n",
    "    os.makedirs(join('output', f'sub-{subject}', f'ses-{session}','label'), exist_ok=True)\n",
    "    os.makedirs(join('output', f'sub-{subject}', f'ses-{session}','t1t2values'), exist_ok=True)\n",
    "    os.makedirs(join('output', f'sub-{subject}', f'ses-{session}','label', 'annotation'), exist_ok=True)\n",
    "    os.makedirs(join('output', f'sub-{subject}', f'ses-{session}','label', 'ROIs'), exist_ok=True)\n",
    "    os.makedirs(join('output', f'sub-{subject}', f'ses-{session}','label', 'FinalLabels'), exist_ok=True)\n",
    "\n",
    "    \n",
    "    bundles = ['ARCL', 'ARCR', 'ATRL', 'ATRR', 'CGCL', 'CGCR', 'CSTL', 'CSTR', 'FA', 'FP', 'IFOL', 'IFOR', 'ILFL', 'ILFR', \n",
    "                  'MdLFL', 'MdLFR', 'ORL', 'ORR', 'pARCL', 'pARCR', 'SLFL', 'SLFR', 'UNCL', 'UNCR', 'VOFL', 'VOFR']    \n",
    "    rightBundles = ['ARCR', 'ATRR', 'CGCR', 'CSTR', 'FA', 'FP', 'IFOR', 'ILFR', 'MdLFR', 'ORR', 'pARCR', 'SLFR', 'UNCR', 'VOFR']\n",
    "    leftBundles = ['ARCL', 'ATRL', 'CGCL', 'CSTL', 'FA', 'FP', 'IFOL', 'ILFL', 'MdLFL', 'ORL', 'pARCL', 'SLFL', 'UNCL', 'VOFL']\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 1: Convert tract files from .trk to .tck\n",
    "    ###############################################################################\n",
    "\n",
    "    for bundle in bundles:\n",
    "        trk2tck = subprocess.run(\n",
    "            [\n",
    "                'nib-trk2tck',\n",
    "                f'output/sub-{subject}/ses-{session}/tracts/sub-{subject}_ses-{session}_coordsys-RASMM_trkmethod-probCSD_recogmethod-AFQ_desc-{bundle}_tractography.trk',\n",
    "            ],\n",
    "            check = True,\n",
    "            capture_output = True,\n",
    "            text=True\n",
    "        )\n",
    "        print_quiet(trk2tck.stdout)\n",
    "        print_quiet(trk2tck.stderr)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 2: Use tckmap to find endpoints in volume space\n",
    "    ###############################################################################\n",
    "    \n",
    "    for bundle in bundles:\n",
    "        tckmap = subprocess.run(\n",
    "            [\n",
    "                'tckmap',\n",
    "                '-template',\n",
    "                f'input/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_T1w.nii.gz',\n",
    "                '-ends_only',\n",
    "                '-info',\n",
    "                '-contrast',\n",
    "                'tdi',\n",
    "                '-force',\n",
    "                f'output/sub-{subject}/ses-{session}/tracts/sub-{subject}_ses-{session}_coordsys-RASMM_trkmethod-probCSD_recogmethod-AFQ_desc-{bundle}_tractography.tck',\n",
    "                f'output/sub-{subject}/ses-{session}/volume/sub-{subject}_ses-{session}_tckmap_bundle-{bundle}.nii.gz'           \n",
    "            ],\n",
    "            check = True,\n",
    "            capture_output = True,\n",
    "            text=True\n",
    "        )\n",
    "        print_quiet(tckmap.stdout)\n",
    "        print_quiet(tckmap.stderr)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 3: Double endpoint-niftii files from uni32 to uni64\n",
    "    ###############################################################################\n",
    "    \n",
    "    for bundle in bundles:\n",
    "        nii = nib.load(os.path.join(f\"output/sub-{subject}/ses-{session}/volume/\", f'sub-{subject}_ses-{session}_tckmap_bundle-{bundle}.nii.gz')) \n",
    "        niidata = nii.get_fdata()\n",
    "        niidouble = np.double(niidata)\n",
    "        niidouble_img = nib.Nifti1Image(niidouble, nii.affine)\n",
    "        nib.save(niidouble_img,f'output/sub-{subject}/ses-{session}/volume/sub-{subject}_ses-{session}_tckmapUNI64_bundle-{bundle}.nii.gz')\n",
    "        \n",
    "    ###############################################################################\n",
    "    # Step 4: Function to use mrivol2surf without freesurfer preprocessing (2nd release)\n",
    "    ###############################################################################\n",
    "\n",
    "    def aligndHCPFreeSurferSurf(ref_volume, trg_surface, work_dir):\n",
    "        ### create working volume directory\n",
    "        mri_dir = op.join(work_dir, \"mri\")\n",
    "        os.makedirs(mri_dir, exist_ok = True)\n",
    "\n",
    "        ### convert reference volume into freesurfer mgz\n",
    "        nib.save(nib.load(ref_volume), op.join(mri_dir, \"ref.mgz\"))\n",
    "        ref_volume = nib.load(op.join(mri_dir, \"ref.mgz\"))\n",
    "        xyz = ref_volume.header.get(\"Pxyz_c\") # volume center\n",
    "\n",
    "        ### create working surface directory\n",
    "        surf_dir = op.join(work_dir, \"surf\")\n",
    "        os.makedirs(surf_dir, exist_ok = True)\n",
    "\n",
    "        for trg_fname in trg_surface: # for each surface file\n",
    "            ### load coordinates and faces of surface file\n",
    "            coords, faces = nib.load(trg_fname).agg_data()\n",
    "\n",
    "            ### center surface vertices\n",
    "            coords = np.apply_along_axis(lambda x: coords - x, -1, xyz)   \n",
    "\n",
    "            ### rotate surface vertices\n",
    "            theta = np.deg2rad(-90) # rotation degrees \n",
    "            R = np.array( # rotation matrix\n",
    "                [[1, 0, 0], \n",
    "                 [0, np.cos(theta), -np.sin(theta)],\n",
    "                 [0, np.sin(theta),  np.cos(theta)]]\n",
    "            )\n",
    "            coords = np.matmul(R, coords.T).T\n",
    "\n",
    "            ### save coregistered surface file\n",
    "            hemi_str = \"lh\" if \"hemi-left\" in trg_fname else \"rh\"\n",
    "            suffix = re.sub(\"\\..+\", \"\", trg_fname.split(\"_\")[-1])\n",
    "            nib.freesurfer.io.write_geometry(\n",
    "                op.join(surf_dir, f\"{hemi_str}.{suffix}\"), coords, faces)\n",
    "            \n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 5: Running mrivol2surf on left hemisphere\n",
    "    ###############################################################################\n",
    "\n",
    "    proj_values=np.arange(-3,3.5,0.5)\n",
    "    ref_volume=f'input/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_T1w.nii.gz'\n",
    "    trg_surface=[f'input/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_hemi-left_wm.surf.gii', f'input/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_hemi-right_wm.surf.gii'] \n",
    "    work_dir=f'output/sub-{subject}/ses-{session}/'\n",
    "\n",
    "    for bundle in leftBundles:\n",
    "        for p in proj_values:\n",
    "            main(ref_volume, trg_surface, work_dir)\n",
    "            mrivol2surf = subprocess.run(\n",
    "                [\n",
    "                    'mri_vol2surf',\n",
    "                    '--sd',\n",
    "                    'output/',\n",
    "                    '--o',\n",
    "                    f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_{p}.mgh',\n",
    "                    '--regheader',\n",
    "                    f'sub-{subject}/ses-{session}',\n",
    "                    '--hemi',\n",
    "                    'lh',\n",
    "                    '--surf',\n",
    "                    'wm',\n",
    "                    '--mov',\n",
    "                    f'output/sub-{subject}/ses-{session}/volume/sub-{subject}_ses-{session}_tckmapUNI64_bundle-{bundle}.nii.gz',\n",
    "                    '--ref',\n",
    "                    'ref.mgz',\n",
    "                    '--projdist',\n",
    "                    f'{p}',\n",
    "                    '--fwhm',\n",
    "                    '1'         \n",
    "                ],\n",
    "                #check = True,\n",
    "                capture_output = True,\n",
    "                text=True\n",
    "                )\n",
    "            print(mrivol2surf.stdout)\n",
    "            print(mrivol2surf.stderr)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 6: Running mrivol2surf on right hemisphere\n",
    "    ###############################################################################\n",
    "\n",
    "    for bundle in rightBundles:\n",
    "        for p in proj_values:\n",
    "            main(ref_volume, trg_surface, work_dir)\n",
    "            mrivol2surf = subprocess.run(\n",
    "                [\n",
    "                    'mri_vol2surf',\n",
    "                    '--sd',\n",
    "                    'output/',\n",
    "                    '--o',\n",
    "                    f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_{p}.mgh',\n",
    "                    '--regheader',\n",
    "                    f'sub-{subject}/ses-{session}',\n",
    "                    '--hemi',\n",
    "                    'rh',\n",
    "                    '--surf',\n",
    "                    'wm',\n",
    "                    '--mov',\n",
    "                    f'output/sub-{subject}/ses-{session}/volume/sub-{subject}_ses-{session}_tckmapUNI64_bundle-{bundle}.nii.gz',\n",
    "                    '--ref',\n",
    "                    'ref.mgz',\n",
    "                    '--projdist',\n",
    "                    f'{p}',\n",
    "                    '--fwhm',\n",
    "                    '1'         \n",
    "                ],\n",
    "                #check = True,\n",
    "                capture_output = True,\n",
    "                text=True\n",
    "                )\n",
    "            print_quiet(mrivol2surf.stdout)\n",
    "            print_quiet(mrivol2surf.stderr)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 7: Running mriconcat on left hemisphere to find maximum endpoints\n",
    "    ###############################################################################\n",
    "    \n",
    "    for bundle in leftBundles:\n",
    "        filepath = f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_bundle-{bundle}_lh_proj_max.mgh'\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "        else:\n",
    "            print(\"Can not delete the file as it doesn't exist\")\n",
    "            \n",
    "        mriconcat = subprocess.run(\n",
    "            [\n",
    "                'mri_concat',\n",
    "                '--i',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_-3.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_-2.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_-2.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_-1.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_-1.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_-0.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_0.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_0.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_1.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_1.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_2.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_2.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_3.0.mgh',                                                                                                                      \n",
    "                '--o',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_bundle-{bundle}_lh_proj_max.mgh',\n",
    "                '--max'\n",
    "            ],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "            )\n",
    "        \n",
    "        for p in proj_values:\n",
    "            os.remove(f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_lh_surface_{p}.mgh')\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 8: Running mriconcat on right hemisphere to find maximum endpoints\n",
    "    ###############################################################################\n",
    "    \n",
    "    for bundle in rightBundles:\n",
    "        filepath = f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_bundle-{bundle}_rh_proj_max.mgh'\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "        else:\n",
    "            print(\"Can not delete the file as it doesn't exist\")\n",
    "    \n",
    "        mriconcat = subprocess.run(\n",
    "            [\n",
    "                'mri_concat',\n",
    "                '--i',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_-3.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_-2.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_-2.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_-1.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_-1.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_-0.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_0.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_0.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_1.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_1.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_2.0.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_2.5.mgh',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_3.0.mgh',\n",
    "                '--o',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_bundle-{bundle}_rh_proj_max.mgh',\n",
    "                '--max'\n",
    "            ],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "            )\n",
    "        print_quiet(mriconcat.stdout)\n",
    "        print_quiet(mriconcat.stderr)\n",
    "        \n",
    "        for p in proj_values:\n",
    "            os.remove(f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_mrivol2surf_bundle-{bundle}_rh_surface_{p}.mgh')\n",
    "            \n",
    "    ###############################################################################\n",
    "    # Step 9: converting dHCP-surface files to freesurfer surface files left hemisphere\n",
    "    ###############################################################################\n",
    "\n",
    "    mrisconvert = subprocess.run(\n",
    "        [\n",
    "            'mris_convert',\n",
    "            f'input/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_hemi-left_wm.surf.gii',\n",
    "            f'output/sub-{subject}/ses-{session}/surf/lh.white'\n",
    "        ],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "        )\n",
    "    print_quiet(mrisconvert.stdout)\n",
    "    print_quiet(mrisconvert.stderr)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 10: converting dHCP-surface files to freesurfer surface files right hemisphere\n",
    "    ###############################################################################\n",
    "\n",
    "    mrisconvert = subprocess.run(\n",
    "        [\n",
    "            'mris_convert',\n",
    "            f'input/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_hemi-right_wm.surf.gii',\n",
    "            f'output/sub-{subject}/ses-{session}/surf/rh.white'\n",
    "        ],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "        )\n",
    "    print_quiet(mrisconvert.stdout)\n",
    "    print_quiet(mrisconvert.stderr)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 11: Load in annotation file and split it into different labels for left hemi\n",
    "    ###############################################################################\n",
    "\n",
    "    annotation_labelLH = subprocess.run(\n",
    "        [\n",
    "            'mri_annotation2label',\n",
    "            '--subject',\n",
    "            f'sub-{subject}/ses-{session}',\n",
    "            '--sd',\n",
    "            'output',\n",
    "            '--hemi',\n",
    "            'lh',\n",
    "            '--annotation',\n",
    "            f'output/sub-{subject}/ses-{session}/label/sub-{subject}_ses-{session}_hemi-left_desc-drawem_dseg.label.gii',\n",
    "            '--outdir',\n",
    "            f'output/sub-{subject}/ses-{session}/label/annotation'\n",
    "        ],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "        )\n",
    "\n",
    "    print_quiet(annotation_labelLH.stdout)\n",
    "    print_quiet(annotation_labelLH.stderr)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 12: Load in annotation file and split it into different labels for right hemi\n",
    "    ###############################################################################\n",
    "\n",
    "    annotation_labelRH = subprocess.run(\n",
    "        [\n",
    "            'mri_annotation2label',\n",
    "            '--subject',\n",
    "            f'sub-{subject}/ses-{session}',\n",
    "            '--sd',\n",
    "            'output',\n",
    "            '--hemi',\n",
    "            'rh',\n",
    "            '--annotation',\n",
    "            f'output/sub-{subject}/ses-{session}/label/sub-{subject}_ses-{session}_hemi-right_desc-drawem_dseg.label.gii',\n",
    "            '--outdir',\n",
    "            f'output/sub-{subject}/ses-{session}/label/annotation'\n",
    "        ],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "        )\n",
    "    print_quiet(annotation_labelRH.stdout)\n",
    "    print_quiet(annotation_labelRH.stderr)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 13: Function to insert text within .label file\n",
    "    ###############################################################################\n",
    "\n",
    "    def prepend_multiple_lines(file_name, list_of_lines):\n",
    "        \"\"\"Insert given list of strings as a new lines at the beginning of a file\"\"\"\n",
    "    \n",
    "        # define name of temporary dummy file\n",
    "        dummy_file = file_name + '.bak'\n",
    "        # open given original file in read mode and dummy file in write mode\n",
    "        with open(file_name, 'r') as read_obj, open(dummy_file, 'w') as write_obj:\n",
    "            # Iterate over the given list of strings and write them to dummy file as lines\n",
    "            for line in list_of_lines:\n",
    "                write_obj.write(line + '\\n')\n",
    "            # Read lines from original file one by one and append them to the dummy file\n",
    "            for line in read_obj:\n",
    "                write_obj.write(line)\n",
    "    \n",
    "        # remove original file\n",
    "        os.remove(file_name)\n",
    "        # Rename dummy file as the original file\n",
    "        os.rename(dummy_file, file_name)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 14: Load in labels for left hemisphere and save them in different ROIs\n",
    "    ###############################################################################\n",
    "\n",
    "    outfolder = f\"output/sub-{subject}/ses-{session}/label/ROIs\"\n",
    "         \n",
    "    labelTable0 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.???.label', skiprows=[0,1], sep='  ', header=None, engine='python')          \n",
    "    labelTable0[4] = [x.split(' ')[0] for x in labelTable0[3]]\n",
    "    labelTable0[5] = [x.split(' ')[-1] for x in labelTable0[3]]\n",
    "    labelTable0 = labelTable0.drop(columns=3)\n",
    "        \n",
    "    labelTable1 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Anterior_temporal_lobe_medial.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable1[4] = [x.split(' ')[0] for x in labelTable1[3]]\n",
    "    labelTable1[5] = [x.split(' ')[-1] for x in labelTable1[3]]\n",
    "    labelTable1 = labelTable1.drop(columns=3)\n",
    "        \n",
    "    labelTable2 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Anterior_temporal_lobe_lateral.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable2[4] = [x.split(' ')[0] for x in labelTable2[3]]\n",
    "    labelTable2[5] = [x.split(' ')[-1] for x in labelTable2[3]]\n",
    "    labelTable2 = labelTable2.drop(columns=3)\n",
    "        \n",
    "    labelTable3 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Gyri_parahippocampalis_et_ambiens_anterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable3[4] = [x.split(' ')[0] for x in labelTable3[3]]\n",
    "    labelTable3[5] = [x.split(' ')[-1] for x in labelTable3[3]]\n",
    "    labelTable3 = labelTable3.drop(columns=3)\n",
    "        \n",
    "    labelTable4 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Superior_temporal_gyrus_middle.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable4[4] = [x.split(' ')[0] for x in labelTable4[3]]\n",
    "    labelTable4[5] = [x.split(' ')[-1] for x in labelTable4[3]]\n",
    "    labelTable4 = labelTable4.drop(columns=3)\n",
    "        \n",
    "    labelTable5 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Medial_and_inferior_temporal_gyri_anterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable5[4] = [x.split(' ')[0] for x in labelTable5[3]]\n",
    "    labelTable5[5] = [x.split(' ')[-1] for x in labelTable5[3]]\n",
    "    labelTable5 = labelTable5.drop(columns=3)\n",
    "        \n",
    "    labelTable6 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Lateral_occipitotemporal_gyrus_gyrus_fusiformis_anterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable6[4] = [x.split(' ')[0] for x in labelTable6[3]]\n",
    "    labelTable6[5] = [x.split(' ')[-1] for x in labelTable6[3]]\n",
    "    labelTable6 = labelTable6.drop(columns=3)\n",
    "        \n",
    "    labelTable7 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Insula.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable7[4] = [x.split(' ')[0] for x in labelTable7[3]]\n",
    "    labelTable7[5] = [x.split(' ')[-1] for x in labelTable7[3]]\n",
    "    labelTable7 = labelTable7.drop(columns=3)\n",
    "        \n",
    "    labelTable8 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Occipital_lobe.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable8[4] = [x.split(' ')[0] for x in labelTable8[3]]\n",
    "    labelTable8[5] = [x.split(' ')[-1] for x in labelTable8[3]]\n",
    "    labelTable8 = labelTable8.drop(columns=3)\n",
    "        \n",
    "    labelTable9 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Gyri_parahippocampalis_et_ambiens_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable9[4] = [x.split(' ')[0] for x in labelTable9[3]]\n",
    "    labelTable9[5] = [x.split(' ')[-1] for x in labelTable9[3]]\n",
    "    labelTable9 = labelTable9.drop(columns=3)\n",
    "        \n",
    "    labelTable10 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Lateral_occipitotemporal_gyrus_gyrus_fusiformis_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable10[4] = [x.split(' ')[0] for x in labelTable10[3]]\n",
    "    labelTable10[5] = [x.split(' ')[-1] for x in labelTable10[3]]\n",
    "    labelTable10 = labelTable10.drop(columns=3)\n",
    "        \n",
    "    labelTable11 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Medial_and_inferior_temporal_gyri_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable11[4] = [x.split(' ')[0] for x in labelTable11[3]]\n",
    "    labelTable11[5] = [x.split(' ')[-1] for x in labelTable11[3]]\n",
    "    labelTable11 = labelTable11.drop(columns=3)\n",
    "        \n",
    "    labelTable12 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Superior_temporal_gyrus_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable12[4] = [x.split(' ')[0] for x in labelTable12[3]]\n",
    "    labelTable12[5] = [x.split(' ')[-1] for x in labelTable12[3]]\n",
    "    labelTable12 = labelTable12.drop(columns=3)\n",
    "        \n",
    "    labelTable13 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Cingulate_gyrus_anterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable13[4] = [x.split(' ')[0] for x in labelTable13[3]]\n",
    "    labelTable13[5] = [x.split(' ')[-1] for x in labelTable13[3]]\n",
    "    labelTable13 = labelTable13.drop(columns=3)\n",
    "        \n",
    "    labelTable14 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Cingulate_gyrus_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable14[4] = [x.split(' ')[0] for x in labelTable14[3]]\n",
    "    labelTable14[5] = [x.split(' ')[-1] for x in labelTable14[3]]\n",
    "    labelTable14 = labelTable14.drop(columns=3)\n",
    "        \n",
    "    labelTable15 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Frontal_lobe.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable15[4] = [x.split(' ')[0] for x in labelTable15[3]]\n",
    "    labelTable15[5] = [x.split(' ')[-1] for x in labelTable15[3]]\n",
    "    labelTable15 = labelTable15.drop(columns=3)\n",
    "        \n",
    "    labelTable16 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/lh.L_Parietal_lobe.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable16[4] = [x.split(' ')[0] for x in labelTable16[3]]\n",
    "    labelTable16[5] = [x.split(' ')[-1] for x in labelTable16[3]]\n",
    "    labelTable16 = labelTable16.drop(columns=3)\n",
    "        \n",
    "    #ARC\n",
    "    labelTable15.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_ARCL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    LabelTableDfs_ARC_L = [labelTable2, labelTable4, labelTable5, labelTable6, labelTable10, labelTable11]\n",
    "    LabelTable_ARC_L_02 = pd.concat(LabelTableDfs_ARC_L)\n",
    "    LabelTable_ARC_L_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_ARCL_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #ATR\n",
    "    labelTable15.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_ATRL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "                \n",
    "    #CGC\n",
    "    LabelTableDfs_CGC_L = [labelTable13, labelTable15]\n",
    "    LabelTable_CGC_L_01 = pd.concat(LabelTableDfs_CGC_L)\n",
    "    LabelTable_CGC_L_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_CGCL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_CGC_L_02 = [labelTable9, labelTable14, labelTable16]\n",
    "    LabelTable_CGC_L_02 = pd.concat(LabelTableDfs_CGC_L_02)\n",
    "    LabelTable_CGC_L_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_CGCL_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #CST\n",
    "    LabelTableDfs_CST_L = [labelTable12, labelTable15, labelTable16]\n",
    "    LabelTable_CST_L_01 = pd.concat(LabelTableDfs_CST_L)\n",
    "    LabelTable_CST_L_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_CSTL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "                        \n",
    "    #FA\n",
    "    labelTable15.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_FA_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "                     \n",
    "    #FP\n",
    "    LabelTableDfs_FP_L = [labelTable8, labelTable16]\n",
    "    LabelTable_FP_L_01 = pd.concat(LabelTableDfs_FP_L)\n",
    "    LabelTable_FP_L_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_FP_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "                        \n",
    "    #IFO\n",
    "    labelTable15.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_IFOL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "          \n",
    "    labelTable8.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_IFOL_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #ILF\n",
    "    LabelTableDfs_ILF_L = [labelTable1, labelTable2, labelTable4, labelTable5]\n",
    "    LabelTable_ILF_L_01 = pd.concat(LabelTableDfs_ILF_L)\n",
    "    LabelTable_ILF_L_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_ILFL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_ILF_L_02 = [labelTable8, labelTable10, labelTable11, labelTable16]\n",
    "    LabelTable_ILF_L_02 = pd.concat(LabelTableDfs_ILF_L_02)\n",
    "    LabelTable_ILF_L_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_ILFL_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #MdLF\n",
    "    LabelTableDfs_MdLF_L = [labelTable1, labelTable2, labelTable3, labelTable4, labelTable5]\n",
    "    LabelTable_MdLF_L_01 = pd.concat(LabelTableDfs_MdLF_L)\n",
    "    LabelTable_MdLF_L_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_MdLFL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_MdLF_L_02 = [labelTable8, labelTable16]\n",
    "    LabelTable_MdLF_L_02 = pd.concat(LabelTableDfs_MdLF_L_02)\n",
    "    LabelTable_MdLF_L_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_MdLFL_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    #OR            \n",
    "    labelTable8.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_ORL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    #pARC\n",
    "    LabelTableDfs_pARC_L = [labelTable12, labelTable11, labelTable5, labelTable10, labelTable6, labelTable8]\n",
    "    LabelTable_pARC_L_01 = pd.concat(LabelTableDfs_pARC_L)\n",
    "    LabelTable_pARC_L_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_pARCL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    labelTable16.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_pARCL_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #SLF\n",
    "    LabelTableDfs_SLF_L = [labelTable7, labelTable15]\n",
    "    LabelTable_SLF_L_01 = pd.concat(LabelTableDfs_SLF_L)\n",
    "    LabelTable_SLF_L_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_SLFL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_SLF_L_02 = [labelTable12, labelTable16]\n",
    "    LabelTable_SLF_L_02 = pd.concat(LabelTableDfs_SLF_L_02)\n",
    "    LabelTable_SLF_L_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_SLFL_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #UNC\n",
    "    LabelTableDfs_UNC_L = [labelTable7, labelTable15]\n",
    "    LabelTable_UNC_L_01 = pd.concat(LabelTableDfs_UNC_L)\n",
    "    LabelTable_UNC_L_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_UNCL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_UNC_L_02 = [labelTable1, labelTable2]\n",
    "    LabelTable_UNC_L_02 = pd.concat(LabelTableDfs_UNC_L_02)\n",
    "    LabelTable_UNC_L_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_UNCL_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    #VOF\n",
    "    LabelTableDfs_VOF_L = [labelTable11, labelTable10, labelTable8]\n",
    "    LabelTable_VOF_L_01 = pd.concat(LabelTableDfs_VOF_L)\n",
    "    LabelTable_VOF_L_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_VOFL_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    labelTable16.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_VOFL_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 15: Load in labels for right hemisphere and save them in different ROIs\n",
    "    ###############################################################################\n",
    "         \n",
    "    labelTable0 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.???.label', skiprows=[0,1], sep='  ', header=None, engine='python')          \n",
    "    labelTable0[4] = [x.split(' ')[0] for x in labelTable0[3]]\n",
    "    labelTable0[5] = [x.split(' ')[-1] for x in labelTable0[3]]\n",
    "    labelTable0 = labelTable0.drop(columns=3)\n",
    "        \n",
    "    labelTable1 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Anterior_temporal_lobe_medial.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable1[4] = [x.split(' ')[0] for x in labelTable1[3]]\n",
    "    labelTable1[5] = [x.split(' ')[-1] for x in labelTable1[3]]\n",
    "    labelTable1 = labelTable1.drop(columns=3)\n",
    "        \n",
    "    labelTable2 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Anterior_temporal_lobe_lateral.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable2[4] = [x.split(' ')[0] for x in labelTable2[3]]\n",
    "    labelTable2[5] = [x.split(' ')[-1] for x in labelTable2[3]]\n",
    "    labelTable2 = labelTable2.drop(columns=3)\n",
    "        \n",
    "    labelTable3 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Gyri_parahippocampalis_et_ambiens_anterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable3[4] = [x.split(' ')[0] for x in labelTable3[3]]\n",
    "    labelTable3[5] = [x.split(' ')[-1] for x in labelTable3[3]]\n",
    "    labelTable3 = labelTable3.drop(columns=3)\n",
    "        \n",
    "    labelTable4 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Superior_temporal_gyrus_middle.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable4[4] = [x.split(' ')[0] for x in labelTable4[3]]\n",
    "    labelTable4[5] = [x.split(' ')[-1] for x in labelTable4[3]]\n",
    "    labelTable4 = labelTable4.drop(columns=3)\n",
    "        \n",
    "    labelTable5 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Medial_and_inferior_temporal_gyri_anterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable5[4] = [x.split(' ')[0] for x in labelTable5[3]]\n",
    "    labelTable5[5] = [x.split(' ')[-1] for x in labelTable5[3]]\n",
    "    labelTable5 = labelTable5.drop(columns=3)\n",
    "        \n",
    "    labelTable6 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Lateral_occipitotemporal_gyrus_gyrus_fusiformis_anterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable6[4] = [x.split(' ')[0] for x in labelTable6[3]]\n",
    "    labelTable6[5] = [x.split(' ')[-1] for x in labelTable6[3]]\n",
    "    labelTable6 = labelTable6.drop(columns=3)\n",
    "        \n",
    "    labelTable7 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Insula.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable7[4] = [x.split(' ')[0] for x in labelTable7[3]]\n",
    "    labelTable7[5] = [x.split(' ')[-1] for x in labelTable7[3]]\n",
    "    labelTable7 = labelTable7.drop(columns=3)\n",
    "        \n",
    "    labelTable8 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Occipital_lobe.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable8[4] = [x.split(' ')[0] for x in labelTable8[3]]\n",
    "    labelTable8[5] = [x.split(' ')[-1] for x in labelTable8[3]]\n",
    "    labelTable8 = labelTable8.drop(columns=3)\n",
    "        \n",
    "    labelTable9 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Gyri_parahippocampalis_et_ambiens_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable9[4] = [x.split(' ')[0] for x in labelTable9[3]]\n",
    "    labelTable9[5] = [x.split(' ')[-1] for x in labelTable9[3]]\n",
    "    labelTable9 = labelTable9.drop(columns=3)\n",
    "        \n",
    "    labelTable10 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Lateral_occipitotemporal_gyrus_gyrus_fusiformis_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable10[4] = [x.split(' ')[0] for x in labelTable10[3]]\n",
    "    labelTable10[5] = [x.split(' ')[-1] for x in labelTable10[3]]\n",
    "    labelTable10 = labelTable10.drop(columns=3)\n",
    "        \n",
    "    labelTable11 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Medial_and_inferior_temporal_gyri_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable11[4] = [x.split(' ')[0] for x in labelTable11[3]]\n",
    "    labelTable11[5] = [x.split(' ')[-1] for x in labelTable11[3]]\n",
    "    labelTable11 = labelTable11.drop(columns=3)\n",
    "        \n",
    "    labelTable12 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Superior_temporal_gyrus_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable12[4] = [x.split(' ')[0] for x in labelTable12[3]]\n",
    "    labelTable12[5] = [x.split(' ')[-1] for x in labelTable12[3]]\n",
    "    labelTable12 = labelTable12.drop(columns=3)\n",
    "        \n",
    "    labelTable13 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Cingulate_gyrus_anterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable13[4] = [x.split(' ')[0] for x in labelTable13[3]]\n",
    "    labelTable13[5] = [x.split(' ')[-1] for x in labelTable13[3]]\n",
    "    labelTable13 = labelTable13.drop(columns=3)\n",
    "        \n",
    "    labelTable14 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Cingulate_gyrus_posterior.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable14[4] = [x.split(' ')[0] for x in labelTable14[3]]\n",
    "    labelTable14[5] = [x.split(' ')[-1] for x in labelTable14[3]]\n",
    "    labelTable14 = labelTable14.drop(columns=3)\n",
    "       \n",
    "    labelTable15 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Frontal_lobe.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable15[4] = [x.split(' ')[0] for x in labelTable15[3]]\n",
    "    labelTable15[5] = [x.split(' ')[-1] for x in labelTable15[3]]\n",
    "    labelTable15 = labelTable15.drop(columns=3)\n",
    "        \n",
    "    labelTable16 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/annotation/rh.R_Parietal_lobe.label', skiprows=[0,1], sep='  ', header=None, engine='python')\n",
    "    labelTable16[4] = [x.split(' ')[0] for x in labelTable16[3]]\n",
    "    labelTable16[5] = [x.split(' ')[-1] for x in labelTable16[3]]\n",
    "    labelTable16 = labelTable16.drop(columns=3)\n",
    "\n",
    "    #ARC\n",
    "    labelTable15.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_ARCR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_ARC_R = [labelTable2, labelTable4, labelTable5, labelTable6, labelTable10, labelTable11]\n",
    "    LabelTable_ARC_R_02 = pd.concat(LabelTableDfs_ARC_R)\n",
    "    LabelTable_ARC_R_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_ARCR_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #ATR\n",
    "    labelTable15.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_ATRR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "                   \n",
    "    #CGC\n",
    "    LabelTableDfs_CGC_R = [labelTable13, labelTable15]\n",
    "    LabelTable_CGC_R_01 = pd.concat(LabelTableDfs_CGC_R)\n",
    "    LabelTable_CGC_R_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_CGCR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_CGC_R_02 = [labelTable9, labelTable14, labelTable16]\n",
    "    LabelTable_CGC_R_02 = pd.concat(LabelTableDfs_CGC_R_02)\n",
    "    LabelTable_CGC_R_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_CGCR_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #CST\n",
    "    LabelTableDfs_CST_R = [labelTable12, labelTable15, labelTable16]\n",
    "    LabelTable_CST_R_01 = pd.concat(LabelTableDfs_CST_R)\n",
    "    LabelTable_CST_R_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_CSTR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "                        \n",
    "    #FA\n",
    "    labelTable15.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_FA_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "                     \n",
    "    #FP\n",
    "    LabelTableDfs_FP_R = [labelTable8, labelTable16]\n",
    "    LabelTable_FP_R_01 = pd.concat(LabelTableDfs_FP_R)\n",
    "    LabelTable_FP_R_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_FP_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "                        \n",
    "    #IFO\n",
    "    labelTable15.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_IFOR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "        \n",
    "    labelTable8.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_IFOR_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #ILF\n",
    "    LabelTableDfs_ILF_R = [labelTable1, labelTable2, labelTable4, labelTable5]\n",
    "    LabelTable_ILF_R_01 = pd.concat(LabelTableDfs_ILF_R)\n",
    "    LabelTable_ILF_R_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_ILFR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_ILF_R_02 = [labelTable8, labelTable10, labelTable11, labelTable16]\n",
    "    LabelTable_ILF_R_02 = pd.concat(LabelTableDfs_ILF_R_02)\n",
    "    LabelTable_ILF_R_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_ILFR_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #MdLF\n",
    "    LabelTableDfs_MdLF_R = [labelTable1, labelTable2, labelTable3, labelTable4, labelTable5]\n",
    "    LabelTable_MdLF_R_01 = pd.concat(LabelTableDfs_MdLF_R)\n",
    "    LabelTable_MdLF_R_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_MdLFR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_MdLF_R_02 = [labelTable8, labelTable16]\n",
    "    LabelTable_MdLF_R_02 = pd.concat(LabelTableDfs_MdLF_R_02)\n",
    "    LabelTable_MdLF_R_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_MdLFR_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    #OR\n",
    "    labelTable8.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_ORR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    #pARC\n",
    "    LabelTableDfs_pARC_R = [labelTable12, labelTable11, labelTable5, labelTable10, labelTable6, labelTable8]\n",
    "    LabelTable_pARC_R_01 = pd.concat(LabelTableDfs_pARC_R)\n",
    "    LabelTable_pARC_R_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_pARCR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    labelTable16.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_pARCR_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #SLF\n",
    "    LabelTableDfs_SLF_R = [labelTable7, labelTable15]\n",
    "    LabelTable_SLF_R_01 = pd.concat(LabelTableDfs_SLF_R)\n",
    "    LabelTable_SLF_R_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_SLFR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_SLF_R_02 = [labelTable12, labelTable16]\n",
    "    LabelTable_SLF_R_02 = pd.concat(LabelTableDfs_SLF_R_02)\n",
    "    LabelTable_SLF_R_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_SLFR_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    #UNC\n",
    "    LabelTableDfs_UNC_R = [labelTable7, labelTable15]\n",
    "    LabelTable_UNC_R_01 = pd.concat(LabelTableDfs_UNC_R)\n",
    "    LabelTable_UNC_R_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_UNCR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "    LabelTableDfs_UNC_R_02 = [labelTable1, labelTable2]\n",
    "    LabelTable_UNC_R_02 = pd.concat(LabelTableDfs_UNC_R_02)\n",
    "    LabelTable_UNC_R_02.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_UNCR_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    #VOF\n",
    "    LabelTableDfs_VOF_R = [labelTable11, labelTable10, labelTable8]\n",
    "    LabelTable_VOF_R_01 = pd.concat(LabelTableDfs_VOF_R)\n",
    "    LabelTable_VOF_R_01.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_VOFR_01.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    labelTable16.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_VOFR_02.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 16: Merge surface endpoint file and label together by vertices to get endpoint-vertices\n",
    "    ###############################################################################\n",
    "\n",
    "    leftBundles = ['ARCL', 'ATRL', 'CGCL', 'CSTL', 'FA', 'FP', 'IFOL', 'ILFL', 'MdLFL', 'ORL', 'pARCL', 'SLFL', 'UNCL', 'VOFL']\n",
    "    outfolder = f\"output/sub-{subject}/ses-{session}/label/FinalLabels\"\n",
    "\n",
    "    for bundle in leftBundles:\n",
    "        endpoints = nib.freesurfer.mghformat.load(f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_bundle-{bundle}_lh_proj_max.mgh')\n",
    "        endpointValues = endpoints.get_fdata()\n",
    "        dfEndpointMap=pd.DataFrame(endpointValues[:,0])\n",
    "        dfEndpointMap.columns=['endpointValues']\n",
    "        dfEndpointMap['verticeNr'] = dfEndpointMap.index\n",
    "        dfEndpointMap['verticeNr'] = dfEndpointMap['verticeNr'].astype(str)\n",
    "            \n",
    "        labelTableBundle01 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/ROIs/sub-{subject}_ses-{session}_LH_{bundle}_01.label', sep=' ', header=None)\n",
    "        labelTableBundle01.columns =['verticeNr', 'x', 'y', 'z', 'value']\n",
    "        labelTableBundle01['verticeNr'] = labelTableBundle01['verticeNr'].astype(str)\n",
    "        labelTableMatch1=pd.merge(labelTableBundle01, dfEndpointMap, on='verticeNr')\n",
    "            \n",
    "        labelTableMatch1=labelTableMatch1[labelTableMatch1['endpointValues'] > 0.0001]\n",
    "        labelTableMatch1=labelTableMatch1.drop(['value'], axis=1)\n",
    "        labelTableMatch1=labelTableMatch1.drop_duplicates('verticeNr')\n",
    "        labelTableMatch1.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_{bundle}_01_matched.label', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "        length1 = len(labelTableMatch1.index)\n",
    "        list_of_lines1 = [f'#!ascii label  , from subject sub-{subject}_ses-{session} vox2ras=TkReg', f'{length1}']\n",
    "        prepend_multiple_lines(f\"output/sub-{subject}/ses-{session}/label/FinalLabels/sub-{subject}_ses-{session}_LH_{bundle}_01_matched.label\", list_of_lines1)\n",
    "            \n",
    "        try:\n",
    "            labelTableBundle02 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/ROIs/sub-{subject}_ses-{session}_LH_{bundle}_02.label', sep=' ', header=None)\n",
    "            labelTableBundle02.columns =['verticeNr', 'x', 'y', 'z', 'value']\n",
    "            labelTableBundle02['verticeNr'] = labelTableBundle02['verticeNr'].astype(str)\n",
    "                                           \n",
    "            labelTableMatch2=pd.merge(labelTableBundle02, dfEndpointMap, on='verticeNr')\n",
    "            labelTableMatch2=labelTableMatch2[labelTableMatch2['endpointValues'] > 0.0001]\n",
    "            labelTableMatch2=labelTableMatch2.drop(['value'], axis=1)\n",
    "            labelTableMatch2=labelTableMatch2.drop_duplicates('verticeNr')\n",
    "            labelTableMatch2.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_LH_{bundle}_02_matched.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "            length2 = len(labelTableMatch2.index)\n",
    "            \n",
    "            list_of_lines2 = [f'#!ascii label  , from subject sub-{subject}_ses-{session} vox2ras=TkReg', f'{length2}']\n",
    "            prepend_multiple_lines(f\"output/sub-{subject}/ses-{session}/label/FinalLabels/sub-{subject}_ses-{session}_LH_{bundle}_02_matched.label\", list_of_lines2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    rightBundles = ['ARCR', 'ATRR', 'CGCR', 'CSTR', 'FA', 'FP', 'IFOR', 'ILFR', 'MdLFR', 'ORR', 'pARCR', 'SLFR', 'UNCR', 'VOFR']\n",
    "\n",
    "    for bundle in rightBundles:\n",
    "        endpoints = nib.freesurfer.mghformat.load(f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_bundle-{bundle}_rh_proj_max.mgh')\n",
    "        endpointValues = endpoints.get_fdata()\n",
    "        dfEndpointMap=pd.DataFrame(endpointValues[:,0])\n",
    "        dfEndpointMap.columns=['endpointValues']\n",
    "        dfEndpointMap['verticeNr'] = dfEndpointMap.index\n",
    "        dfEndpointMap['verticeNr'] = dfEndpointMap['verticeNr'].astype(str)\n",
    "            \n",
    "        labelTableBundle01 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/ROIs/sub-{subject}_ses-{session}_RH_{bundle}_01.label', sep=' ', header=None)\n",
    "        labelTableBundle01.columns =['verticeNr', 'x', 'y', 'z', 'value']\n",
    "        labelTableBundle01['verticeNr'] = labelTableBundle01['verticeNr'].astype(str)\n",
    "            \n",
    "        labelTableMatch1=pd.merge(labelTableBundle01, dfEndpointMap, on='verticeNr')\n",
    "        labelTableMatch1=labelTableMatch1[labelTableMatch1['endpointValues'] > 0.0001]\n",
    "        labelTableMatch1=labelTableMatch1.drop(['value'], axis=1)\n",
    "        labelTableMatch1=labelTableMatch1.drop_duplicates('verticeNr')\n",
    "        labelTableMatch1.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_{bundle}_01_matched.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "        length1 = len(labelTableMatch1.index)\n",
    "        list_of_lines1 = [f'#!ascii label  , from subject sub-{subject}_ses-{session} vox2ras=TkReg', f'{length1}']\n",
    "        prepend_multiple_lines(f\"output/sub-{subject}/ses-{session}/label/FinalLabels/sub-{subject}_ses-{session}_RH_{bundle}_01_matched.label\", list_of_lines1)\n",
    "            \n",
    "        try:\n",
    "            labelTableBundle02 = pd.read_table(f'output/sub-{subject}/ses-{session}/label/ROIs/sub-{subject}_ses-{session}_RH_{bundle}_02.label', sep=' ', header=None)\n",
    "            labelTableBundle02.columns =['verticeNr', 'x', 'y', 'z', 'value']\n",
    "            labelTableBundle02['verticeNr'] = labelTableBundle02['verticeNr'].astype(str)\n",
    "                       \n",
    "            labelTableMatch2=pd.merge(labelTableBundle02, dfEndpointMap, on='verticeNr')\n",
    "            labelTableMatch2=labelTableMatch2[labelTableMatch2['endpointValues'] > 0.0001]\n",
    "            labelTableMatch2=labelTableMatch2.drop(['value'], axis=1)\n",
    "            labelTableMatch2=labelTableMatch2.drop_duplicates('verticeNr')\n",
    "            labelTableMatch2.to_csv(f'{outfolder}/sub-{subject}_ses-{session}_RH_{bundle}_02_matched.label', header=None, index=None, sep=' ', mode='a')\n",
    "            \n",
    "            length2 = len(labelTableMatch2.index)\n",
    "            list_of_lines2 = [f'#!ascii label  , from subject sub-{subject}_ses-{session} vox2ras=TkReg', f'{length2}']\n",
    "            prepend_multiple_lines(f\"output/sub-{subject}/ses-{session}/label/FinalLabels/sub-{subject}_ses-{session}_RH_{bundle}_02_matched.label\", list_of_lines2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "     ###############################################################################\n",
    "    # Step 17. Convert surface myelinmap.gii files to .mgh files\n",
    "    ###############################################################################\n",
    "\n",
    "\n",
    "    converttomgh_LH = subprocess.run(\n",
    "        [\n",
    "            'mri_convert',\n",
    "                f'input/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_hemi-left_myelinmap.shape.gii',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_hemi-L_space-T2w_myelinmap.shape.mgh'\n",
    "        ],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "        )\n",
    "    print_quiet(converttomgh_LH.stdout)\n",
    "    print_quiet(converttomgh_LH.stdout)\n",
    "\n",
    "\n",
    "    converttomgh_RH = subprocess.run(\n",
    "        [\n",
    "            'mri_convert',\n",
    "                f'input/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_hemi-right_myelinmap.shape.gii',\n",
    "                f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_hemi-R_space-T2w_myelinmap.shape.mgh'\n",
    "        ],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "        )\n",
    "    print_quiet(converttomgh_RH.stdout)\n",
    "    print_quiet(converttomgh_RH.stdout)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 18: Find myelinvalue in vertices of endpoints (left hemisphere)\n",
    "    ###############################################################################\n",
    "\n",
    "    for bundle in leftBundles:\n",
    "        t1wt2wMap_LH = nib.freesurfer.mghformat.load(f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_hemi-L_space-T2w_myelinmap.shape.mgh')\n",
    "        t1wt2wValues_LH = t1wt2wMap_LH.get_fdata()\n",
    "        t1wt2wValuesMap_LH=pd.DataFrame(t1wt2wValues_LH[:,0])\n",
    "        t1wt2wValuesMap_LH.columns=['t1wt2wValues_LH']\n",
    "        t1wt2wValuesMap_LH['verticeNr'] = t1wt2wValuesMap_LH.index\n",
    "        t1wt2wValuesMap_LH['verticeNr'] = t1wt2wValuesMap_LH['verticeNr'].astype(str)\n",
    "        t1wt2wValuesMap_LH\n",
    "\n",
    "        rois1_LH = pd.read_table(f\"output/sub-{subject}/ses-{session}/label/FinalLabels/sub-{subject}_ses-{session}_LH_{bundle}_01_matched.label\",  skiprows=[0,1], sep=' ', header=None)\n",
    "        rois1_LH.columns =['verticeNr', 'x', 'y', 'z', 'endpointdensity']\n",
    "        rois1_LH['verticeNr'] = rois1_LH['verticeNr'].astype(str)\n",
    "        rois1_LH\n",
    "        t1wt2w_data_rois1_LH=pd.merge(t1wt2wValuesMap_LH, rois1_LH, on='verticeNr')\n",
    "        t1wt2w_data_rois1_LH[\"subjectID\"]=f'{subject}'\n",
    "        t1wt2w_data_rois1_LH[\"sessionID\"]=f'{session}'\n",
    "        t1wt2w_data_rois1_LH[\"tractID\"]=f'{bundle}'\n",
    "        t1wt2w_data_rois1_LH[\"ROI\"]='1'\n",
    "        t1wt2w_data_rois1_LH[\"hemisphere\"]='L'\n",
    "        t1wt2w_data_rois1_LH\n",
    "\n",
    "        try:\n",
    "            rois2_LH = pd.read_table(f\"output/sub-{subject}/ses-{session}/label/FinalLabels/sub-{subject}_ses-{session}_LH_{bundle}_02_matched.label\",  skiprows=[0,1], sep=' ', header=None)\n",
    "            rois2_LH.columns =['verticeNr', 'x', 'y', 'z', 'endpointdensity']\n",
    "            rois2_LH['verticeNr'] = rois2_LH['verticeNr'].astype(str)\n",
    "            rois2_LH\n",
    "            t1wt2w_data_rois2_LH=pd.merge(t1wt2wValuesMap_LH, rois2_LH, on='verticeNr')\n",
    "            t1wt2w_data_rois2_LH[\"subjectID\"]=f'{subject}'\n",
    "            t1wt2w_data_rois2_LH[\"sessionID\"]=f'{session}'\n",
    "            t1wt2w_data_rois2_LH[\"tractID\"]=f'{bundle}'\n",
    "            t1wt2w_data_rois2_LH[\"ROI\"]='2'\n",
    "            t1wt2w_data_rois2_LH[\"hemisphere\"]='L'\n",
    "            t1wt2w_data_rois2_LH\n",
    "\n",
    "            t1wt2wdataLH = pd.concat([t1wt2w_data_rois1_LH, t1wt2w_data_rois2_LH], ignore_index=True)\n",
    "            t1wt2wdataLH\n",
    "            t1wt2wdataLH.to_csv(f'output/sub-{subject}/ses-{session}/t1t2values/GreyMatterT1T2_{bundle}_LH.csv', sep=' ', mode='a')\n",
    "            \n",
    "        except:\n",
    "            t1wt2w_data_rois1_LH.to_csv(f'output/sub-{subject}/ses-{session}/t1t2values/GreyMatterT1T2_{bundle}_LH.csv', sep=' ', mode='a')\n",
    "\n",
    "    ###############################################################################\n",
    "    # Step 19: Find myelinvalue in vertices of endpoints (right hemisphere)\n",
    "    ###############################################################################\n",
    "\n",
    "    for bundle in rightBundles:\n",
    "        t1wt2wMap_RH = nib.freesurfer.mghformat.load(f'output/sub-{subject}/ses-{session}/surf/sub-{subject}_ses-{session}_hemi-R_space-T2w_myelinmap.shape.mgh')\n",
    "        t1wt2wValues_RH = t1wt2wMap_RH.get_fdata()\n",
    "        t1wt2wValuesMap_RH=pd.DataFrame(t1wt2wValues_RH[:,0])\n",
    "        t1wt2wValuesMap_RH.columns=['t1wt2wValues_RH']\n",
    "        t1wt2wValuesMap_RH['verticeNr'] = t1wt2wValuesMap_RH.index\n",
    "        t1wt2wValuesMap_RH['verticeNr'] = t1wt2wValuesMap_RH['verticeNr'].astype(str)\n",
    "        t1wt2wValuesMap_RH\n",
    "\n",
    "        rois1_RH = pd.read_table(f\"output/sub-{subject}/ses-{session}/label/FinalLabels/sub-{subject}_ses-{session}_RH_{bundle}_01_matched.label\",  skiprows=[0,1], sep=' ', header=None)\n",
    "        rois1_RH.columns =['verticeNr', 'x', 'y', 'z', 'endpointdensity']\n",
    "        rois1_RH['verticeNr'] = rois1_RH['verticeNr'].astype(str)\n",
    "        rois1_RH\n",
    "        t1wt2w_data_rois1_RH=pd.merge(t1wt2wValuesMap_RH, rois1_RH, on='verticeNr')\n",
    "        t1wt2w_data_rois1_RH[\"subjectID\"]=f'{subject}'\n",
    "        t1wt2w_data_rois1_RH[\"sessionID\"]=f'{session}'\n",
    "        t1wt2w_data_rois1_RH[\"tractID\"]=f'{bundle}'\n",
    "        t1wt2w_data_rois1_RH[\"ROI\"]='1'\n",
    "        t1wt2w_data_rois1_RH[\"hemisphere\"]='R'\n",
    "        t1wt2w_data_rois1_RH\n",
    "\n",
    "        try:\n",
    "            rois2_RH = pd.read_table(f\"output/sub-{subject}/ses-{session}/label/FinalLabels/sub-{subject}_ses-{session}_RH_{bundle}_02_matched.label\",  skiprows=[0,1], sep=' ', header=None)\n",
    "            rois2_RH.columns =['verticeNr', 'x', 'y', 'z', 'endpointdensity']\n",
    "            rois2_RH['verticeNr'] = rois2_LH['verticeNr'].astype(str)\n",
    "            rois2_RH\n",
    "            t1wt2w_data_rois2_RH=pd.merge(t1wt2wValuesMap_RH, rois2_RH, on='verticeNr')\n",
    "            t1wt2w_data_rois2_RH[\"subjectID\"]=f'{subject}'\n",
    "            t1wt2w_data_rois2_RH[\"sessionID\"]=f'{session}'\n",
    "            t1wt2w_data_rois2_RH[\"tractID\"]=f'{bundle}'\n",
    "            t1wt2w_data_rois2_RH[\"ROI\"]='2'\n",
    "            t1wt2w_data_rois2_RH[\"hemisphere\"]='R'\n",
    "            t1wt2w_data_rois2_RH\n",
    "    \n",
    "            t1wt2wdataRH = pd.concat([t1wt2w_data_rois1_RH, t1wt2w_data_rois2_RH], ignore_index=True)\n",
    "            t1wt2wdataRH\n",
    "            t1wt2wdataRH.to_csv(f'output/sub-{subject}/ses-{session}/t1t2values/GreyMatterT1T2_{bundle}_RH.csv', sep=' ', mode='a')\n",
    "            \n",
    "        except:\n",
    "            t1wt2w_data_rois1_RH.to_csv(f'output/sub-{subject}/ses-{session}/t1t2values/GreyMatterT1T2_{bundle}_RH.csv', sep=' ', mode='a')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
